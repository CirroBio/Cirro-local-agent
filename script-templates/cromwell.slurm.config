
backend {
    default: "AWSBATCH"
    providers {
        AWSBATCH {
            actor-factory: "cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory"
            config {
                // Base bucket for workflow executions
                root: "${PW_S3_WORK}"

                // A reference to an auth defined in the aws stanza at the top. This auth is used to create
                // Jobs and manipulate auth JSONs.
                auth: "default"

                numSubmitAttempts: 5
                numCreateDefinitionAttempts: 5

                concurrent-job-limit: 100

                default-runtime-attributes {
                    queueArn:  "${PW_SPOT_JOB_QUEUE}"
                    scriptBucketName: "${PW_S3_WORK_BUCKET}"
                    logGroupName: "/Cirro/${PW_PROJECT}/job"
                    additionalTags {
                        projectid: "${PW_PROJECT}"
                        datasetid: "${PW_DATASET}"
                    }
                    maxRetries: 3
                }

                filesystems {
                    s3 {
                        // A reference to a potentially different auth for manipulating files via engine functions.
                        auth: "default"
                    }
                }
            }
        }
        SLURM {
            actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
            config {
                glob-link-command = "ln -sL GLOB_PATTERN GLOB_DIRECTORY"
                # For BeeGFS so softlink is used instead of hardlink
                concurrent-job-limit = 5000
                runtime-attributes = """
                Int cpu = 1
                String? walltime
                Int memory_mb = 2000
                String partition = "${PW_ONDEMAND_JOB_QUEUE}"
                String? docker
                String? modules = ""
                String? dockerSL
                String? account
                """

            submit = """
                set -e
                source /app/lmod/lmod/init/bash
                module use /app/modules/all
                module purge

                if [ ! -z '${dockerSL}' ]; then

                    # Ensure Apptainer is loaded if it's installed as a module
                    module load Apptainer/${PW_APPTAINER_VERSION:-1.1.6}
                    # Build the Docker image into a apptainer image
                    DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${dockerSL})
                    # The image will live together with all the other images to force caching of the .sif files themselves - note, always use docker hub tags!!!
                    IMAGE=$PW_APPTAINER_CACHEDIR/$DOCKER_NAME.sif

                    echo $DOCKER_NAME
                    echo $IMAGE
                    echo $PW_APPTAINER_CACHEDIR
                    echo $SCRATCHPATH

                    if [ ! -f $IMAGE ]; then  # If we already have the image, skip everything
                        apptainer pull $IMAGE docker://${dockerSL}
                    fi   

                    # Submit the script to SLURM
                    sbatch \
                        --partition=${partition} \
                        -J ${job_name} \
                        -D ${cwd} \
                        -o ${cwd}/execution/stdout \
                        -e ${cwd}/execution/stderr \
                        --cpus-per-task=${cpu} ${"--time=" + walltime} ${"-A " + account} \
                        --wrap "apptainer exec --bind $SCRATCHPATH $IMAGE ${job_shell} ${script}"

                else
                    module load ${modules}
                    
                    sbatch \
                        --partition=${partition} \
                        -J ${job_name} \
                        -D ${cwd} \
                        -o ${out} \
                        -e ${err} \
                        --cpus-per-task=${cpu} ${"--time=" + walltime} ${"-A " + account} \
                        --wrap "/bin/bash ${script}"

                fi
            """

            submit-docker = """
                set -e
                source /app/lmod/lmod/init/bash
                module use /app/modules/all
                module purge

                # Ensure apptainer is loaded if it's installed as a module
                module load Apptainer/${PW_APPTAINER_VERSION:-1.1.6}
                # Build the Docker image into a apptainer image
                DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker})
                # The image will live together with all the other images to force "caching" of the .sif files themselves - note, always use docker hub tags!!!
                IMAGE=$PW_APPTAINER_CACHEDIR/$DOCKER_NAME.sif

                if [ ! -f $IMAGE ]; then  # If we already have the image, skip everything
                    apptainer pull $IMAGE docker://${docker}
                fi   

                # Submit the script to SLURM
                sbatch \
                --partition=${partition} \
                -J ${job_name} \
                -D ${cwd} \
                -o ${cwd}/execution/stdout \
                -e ${cwd}/execution/stderr \
                --cpus-per-task=${cpu} ${"--time=" + walltime} ${"-A " + account} \
                --wrap "apptainer exec --bind ${cwd}:${docker_cwd} --bind $HOME $IMAGE ${job_shell} ${docker_script}"
            """

            filesystems {
                local {
                    ## for local SLURM, hardlink doesn't work. Options for this and caching: , "soft-link" , "hard-link", "copy"
                    localization: ["soft-link", "copy"]
                    ## call caching config relating to the filesystem side
                    caching {
                        # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:  "hard-link", "soft-link", "copy", "cached-copy".
                        duplication-strategy: [
                            "soft-link", "copy"
                        ]
                        hashing-strategy: "path+modtime"

                        # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.
                        # If false or the md5 does not exist, will proceed with the above-defined hashing strategy.
                        # Default: false
                        check-sibling-md5: false
                    }
                }
            }
        }
    }
}
